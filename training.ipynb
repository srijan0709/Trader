{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQNTrainer:\n",
    "    def __init__(self, model, target_model, device='cuda'):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.target_model = target_model.to(device)\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()  # Huber loss\n",
    "        \n",
    "        # Match notebook's hyperparameters\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.997\n",
    "        self.tau = 0.001  # For soft target updates\n",
    "        self.transaction_cost = 0.0001  # 0.01% per transaction\n",
    "        \n",
    "        # Technical indicators from notebook\n",
    "        self.feature_order = [\n",
    "            'MA50', 'RSI', 'MACD', 'BB_upper', 'BB_lower', \n",
    "            'ADX', 'CCI', 'ATR', 'ROC', 'OBV'\n",
    "        ]\n",
    "\n",
    "    def _normalize_features(self, state):\n",
    "        # Normalization logic matching notebook's approach\n",
    "        normalized = torch.FloatTensor([\n",
    "            state['MA50'] / state['close'],    # Moving average ratio\n",
    "            state['RSI'] / 100,                # RSI normalized 0-1\n",
    "            state['MACD'] * 100,               # MACD scaled\n",
    "            (state['close'] - state['BB_lower']) / (state['BB_upper'] - state['BB_lower'] + 1e-8),\n",
    "            state['ADX'] / 100,                # ADX normalized\n",
    "            state['CCI'] / 200,                # CCI scaled\n",
    "            state['ATR'] / state['close'],     # ATR ratio\n",
    "            state['ROC'] / 100,                # ROC percentage\n",
    "            state['OBV'] / 1e6                 # OBV scaled\n",
    "        ]).to(self.device)\n",
    "        return normalized\n",
    "\n",
    "    def _calculate_reward(self, current_price, next_price, action, position):\n",
    "        # Match notebook's reward calculation\n",
    "        price_change = (next_price - current_price) / current_price\n",
    "        fee = self.transaction_cost\n",
    "        \n",
    "        if action == 0:  # Buy\n",
    "            reward = price_change - fee\n",
    "        elif action == 1:  # Sell\n",
    "            reward = -price_change - fee\n",
    "        else:  # Hold\n",
    "            reward = price_change * position\n",
    "            \n",
    "        return reward * 100  # Scaling factor from notebook\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((\n",
    "            self._normalize_features(state),\n",
    "            action,\n",
    "            reward,\n",
    "            self._normalize_features(next_state) if not done else None,\n",
    "            done\n",
    "        ))\n",
    "\n",
    "    def act(self, state, current_position):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice([0, 1, 2])  # Buy, Sell, Hold\n",
    "        \n",
    "        state_tensor = self._normalize_features(state)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.stack([s for s in next_states if s is not None])\n",
    "        dones = torch.BoolTensor(dones).to(self.device)\n",
    "\n",
    "        # Current Q values\n",
    "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "\n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_model(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones.float()) * self.gamma * next_q\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q.squeeze(), target_q)\n",
    "\n",
    "        # Optimize model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft target network update\n",
    "        for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save({\n",
    "            'model_state': self.model.state_dict(),\n",
    "            'target_state': self.target_model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.target_model.load_state_dict(checkpoint['target_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with input size matching technical indicators\n",
    "from dqnModel import DQN, TargetNetwork\n",
    "input_size = 9  # Number of normalized features\n",
    "model = DQN(input_size)\n",
    "target_model = TargetNetwork(model)\n",
    "trainer = DQNTrainer(model, target_model)\n",
    "\n",
    "# Training loop matching notebook structure\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    position = 0  # Track current position\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = trainer.act(state, position)\n",
    "        next_state, price_change, done = env.step(action)\n",
    "        \n",
    "        # Update position based on action\n",
    "        if action == 0: position = 1    # Buy\n",
    "        elif action == 1: position = -1 # Sell\n",
    "        # Hold maintains current position\n",
    "        \n",
    "        reward = trainer._calculate_reward(\n",
    "            state['close'], \n",
    "            next_state['close'],\n",
    "            action,\n",
    "            position\n",
    "        )\n",
    "        \n",
    "        trainer.remember(state, action, reward, next_state, done)\n",
    "        loss = trainer.replay()\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    if episode % 100 == 0:\n",
    "        trainer.save_checkpoint(f'dqn_checkpoint_{episode}.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
