{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Helper Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "from utils_data import  generateTargetDataBuySide, generateTargetDataSellSide, getTechnicalIndicators, normalize_dataframe_with_mean_std\n",
    "from utils_data import UpstoxStockDataFetcher\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher = UpstoxStockDataFetcher()\n",
    "stock_symbol = \"SUZLON\"\n",
    "start_date = \"2025-06-15\"\n",
    "end_date = \"2025-06-18\"\n",
    "df = fetcher.get_stock_data(stock_symbol, start_date, end_date)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_today = pd.read_csv(r\"C:\\Users\\srija\\Assignment\\Trading\\json_files\\suzlon_2025-06-19.csv\")\n",
    "df_today = df_today.drop_duplicates()\n",
    "df_today['time'] = df_today['time'].astype(str) + \"+05:30\"\n",
    "df_today['time'] = pd.to_datetime(df_today['time'])\n",
    "df = pd.concat([df, df_today], ignore_index=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getTechnicalIndicators(df)\n",
    "target_buy = generateTargetDataBuySide(df,1.005,0.99)\n",
    "target_sell = generateTargetDataSellSide(df,0.995,1.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 = len(target_buy[target_buy['action'].isin(['End of Day'])])\n",
    "count2 = len(target_buy[target_buy['action'].isin(['Target Hit'])])\n",
    "count3 = len(target_buy[target_buy['action'].isin(['Stop Loss Hit'])])\n",
    "\n",
    "print(f\"End of Day: {count1}\")\n",
    "print(f\"Target Hit: {count2}\")\n",
    "print(f\"Stop Loss Hit: {count3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count4 = len(target_sell[target_sell['action'].isin(['End of Day'])])\n",
    "count5 = len(target_sell[target_sell['action'].isin(['Target Hit'])])\n",
    "count6 = len(target_sell[target_sell['action'].isin(['Stop Loss Hit'])])\n",
    "\n",
    "print(f\"End of Day: {count4}\")\n",
    "print(f\"Target Hit: {count5}\")\n",
    "print(f\"Stop Loss Hit: {count6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing and Storing Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized, norm_param = normalize_dataframe_with_mean_std(df)\n",
    "\n",
    "# Storing Norm parameters to be used later for inference\n",
    "with open(\"C:/Users/srija/Assignment/Trading/json_files/suzlon_14_june_norm_params.json\", \"w\") as f:\n",
    "    json.dump(norm_param, f)\n",
    "df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.DQN import DQN,DQNAgent\n",
    "from trading_environment import StockTradingEnv\n",
    "\n",
    "policy_net = DQN(16, 3)\n",
    "target_net = DQN(16, 3)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(df, current_step):\n",
    "    row = df.iloc[current_step]\n",
    "    state = np.array([\n",
    "        row['time'],\n",
    "        row['open'],\n",
    "        row['high'],\n",
    "        row['low'],\n",
    "        row['close'],\n",
    "        row['volume'],\n",
    "        row['MA50'],\n",
    "        row['RSI'],\n",
    "        row['MACD'],\n",
    "        row['BB_upper'],\n",
    "        row['BB_lower'],\n",
    "        row['ADX'],\n",
    "        row['CCI'],\n",
    "        row['ATR'],\n",
    "        row['ROC'],\n",
    "        row['OBV']\n",
    "    ], dtype=np.float32)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = r\"C:\\Users\\srija\\Assignment\\Trading\\Models\\trained_models\\suzlon_14_june\"\n",
    "model_path = r\"C:\\Users\\srija\\Assignment\\Trading\\Models\\trained_models\\suzlon_14_june\\suzlon_14_june_1496.pth\"\n",
    "policy_net.load_state_dict(torch.load(model_path))\n",
    "policy_net.train()\n",
    "env = StockTradingEnv(df_normalized)\n",
    "agent = DQNAgent(env, policy_net, target_net)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "policy_net.to(device)\n",
    "target_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimized_scalping_reward(delay, action_type, success_base_reward=1500, \n",
    "                                     failure_base_penalty=1000, min_delay_threshold=60, \n",
    "                                     max_reward=2500, decay_rate=0.3,\n",
    "                                     opportunity_cost_factor=0.2,\n",
    "                                     missed_opp_multiplier=2.0,\n",
    "                                     consecutive_successes=0, \n",
    "                                     consecutive_success_bonus=0.15):\n",
    "    \"\"\"\n",
    "    Comprehensive reward function optimized for scalping.\n",
    "    \"\"\"\n",
    "    delay = delay/60\n",
    "    if action_type == 'success':\n",
    "        # Delay-dependent base reward scaling\n",
    "        if delay <= min_delay_threshold:\n",
    "            base_reward = max_reward - (max_reward - success_base_reward) * (delay / min_delay_threshold)\n",
    "        else:\n",
    "            base_reward = success_base_reward\n",
    "        \n",
    "        # Apply exponential decay\n",
    "        reward = base_reward * np.exp(-decay_rate * delay)\n",
    "        \n",
    "        # Apply opportunity cost\n",
    "        opportunity_cost = opportunity_cost_factor * delay * success_base_reward\n",
    "        opportunity_cost = min(opportunity_cost, reward * 0.8)\n",
    "        reward = reward - opportunity_cost\n",
    "        \n",
    "        # Apply sequential bonus\n",
    "        if consecutive_successes > 0:\n",
    "            sequential_bonus = reward * (consecutive_success_bonus * consecutive_successes)\n",
    "            reward += sequential_bonus\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    elif action_type == 'failure':\n",
    "        # Standard penalty with exponential decay\n",
    "        penalty = -failure_base_penalty * np.exp(-decay_rate * delay)\n",
    "        \n",
    "        # Add opportunity cost to penalty\n",
    "        opportunity_cost = opportunity_cost_factor * delay * failure_base_penalty\n",
    "        penalty = penalty - opportunity_cost\n",
    "        \n",
    "        return penalty\n",
    "    \n",
    "    elif action_type == 'missed_opportunity':\n",
    "        # Enhanced penalty for missed opportunities\n",
    "        missed_penalty = -failure_base_penalty * missed_opp_multiplier * np.exp(-decay_rate * delay)\n",
    "        return missed_penalty\n",
    "    \n",
    "    elif action_type == 'no_action':\n",
    "        # Reward for correctly staying out of the market\n",
    "        return 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3000\n",
    "\n",
    "for episode in range(1500,num_episodes):\n",
    "    # state = env.reset()\n",
    "    total_reward = 0\n",
    "    number_trans = 0\n",
    "    wins =0\n",
    "    lose = 0\n",
    "    defeat =0\n",
    "    consecutive_success = 0\n",
    "    pbar = tqdm(total = len(df_normalized))\n",
    "    step =0\n",
    "    next_step = 0\n",
    "\n",
    "    while step<len(df_normalized):\n",
    "    \n",
    "        \n",
    "        state = get_state(df_normalized,step)\n",
    "        action = agent.select_action(state)\n",
    "        done = False\n",
    "        # print(action)\n",
    "       \n",
    "        if action ==1: ## BUY\n",
    "            next_state = target_buy.iloc[step]\n",
    "            \n",
    "            next_state_index = next_state[\"next_state_index\"]\n",
    "            \n",
    "            next_state2 = df_normalized.iloc[next_state_index].copy()\n",
    "\n",
    "            reward = 0\n",
    "            if(next_state['action']==\"Target Hit\"):\n",
    "                wins +=1\n",
    "                consecutive_success+=1\n",
    "                # reward = 1000/(target_buy.iloc[step]['delay']+1)\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_buy.iloc[step]['delay'],action_type=\"success\",consecutive_successes=consecutive_success)\n",
    "                \n",
    "              \n",
    "            elif next_state['action']==\"Stop Loss Hit\":\n",
    "                defeat +=1\n",
    "                consecutive_success=0\n",
    "                # reward = -1000/(target_buy.iloc[step]['delay']+1)\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_buy.iloc[step]['delay'],action_type=\"failure\",consecutive_successes=consecutive_success)\n",
    "                \n",
    "                \n",
    "            elif next_state['action']==\"End of Day\":\n",
    "                lose+=1\n",
    "                consecutive_success=0\n",
    "                done = True\n",
    "                # reward = -50\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_buy.iloc[step]['delay'],action_type=\"failure\",consecutive_successes=consecutive_success)\n",
    "                \n",
    "               \n",
    "            reward = float(reward)  # Convert to scalar float\n",
    "            \n",
    "            next_state2 = np.array(next_state2.values, dtype=np.float32)\n",
    "            agent.store_transition(state, action, reward, next_state2, done)\n",
    "            agent.update_policy()\n",
    "            number_trans +=1\n",
    "            next_step = next_state_index+1\n",
    "        \n",
    "        if action==2: ## Sell  short\n",
    "            next_state = target_sell.iloc[step]\n",
    "            next_state_index = next_state[\"next_state_index\"]\n",
    "            \n",
    "            next_state2 = df_normalized.iloc[next_state_index].copy()\n",
    "            reward = 0\n",
    "            if(next_state['action']==\"Target Hit\"):\n",
    "                wins +=1\n",
    "                consecutive_success+=1\n",
    "                # reward = 1000/(target_sell.iloc[step]['delay']+1)\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_sell.iloc[step]['delay'],action_type=\"success\",consecutive_successes=consecutive_success)\n",
    "                \n",
    "               \n",
    "            elif next_state['action']==\"Stop Loss Hit\":\n",
    "                consecutive_success=0\n",
    "                defeat +=1\n",
    "                # reward = -1000/(target_sell.iloc[step]['delay']+1)\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_sell.iloc[step]['delay'],action_type=\"failure\",consecutive_successes=consecutive_success)\n",
    "                \n",
    "               \n",
    "            elif next_state['action']==\"End of Day\":\n",
    "                consecutive_success=0\n",
    "                lose+=1\n",
    "                done =True\n",
    "                # reward = -50\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_sell.iloc[step]['delay'],action_type=\"failure\",consecutive_successes=consecutive_success)\n",
    "                \n",
    "                \n",
    "            reward = float(reward)  # Convert to scalar float\n",
    "           \n",
    "            next_state2 = np.array(next_state2.values, dtype=np.float32)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state2, done)\n",
    "            agent.update_policy()\n",
    "            number_trans +=1\n",
    "            next_step = next_state_index+1\n",
    "        if action ==0:\n",
    "            buy_side = target_buy.iloc[step].copy()\n",
    "            sell_side = target_sell.iloc[step].copy()\n",
    "            if buy_side['action']==\"Target Hit\":\n",
    "                # reward = -1000/(target_buy.iloc[step]['delay']+1)\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_buy.iloc[step]['delay'],action_type=\"missed_opportunity\",consecutive_successes=consecutive_success)     \n",
    "            elif sell_side['action']==\"Target Hit\":\n",
    "                # reward = -1000/(target_sell.iloc[step]['delay']+1)\n",
    "                reward = calculate_optimized_scalping_reward(delay=target_sell.iloc[step]['delay'],action_type=\"missed_opportunity\",consecutive_successes=consecutive_success)\n",
    "            else:\n",
    "                reward = 100\n",
    "            if step+1 < len(df_normalized):\n",
    "               \n",
    "                next_state = get_state(df_normalized,step+1)\n",
    "                \n",
    "                reward = float(reward)  # Convert to scalar float\n",
    "                agent.store_transition(state, action, reward, next_state, done)\n",
    "                agent.update_policy()\n",
    "            else:\n",
    "                done = True\n",
    "                next_state = get_state(df_normalized,-1)\n",
    "                reward = float(reward)  # Convert to scalar float\n",
    "                agent.store_transition(state, action, reward, next_state, done)\n",
    "                agent.update_policy()\n",
    "            next_step = step+1\n",
    "            \n",
    "        pbar.update(next_step - step)\n",
    "        step = next_step\n",
    "    pbar.close()\n",
    "                    \n",
    "    \n",
    "    # Update the target network\n",
    "    if episode % 5 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        model_save_path = os.path.join(save_folder, f'suzlon_14_june_{episode+1}.pth')\n",
    "        print(f'Episode {episode + 1}')\n",
    "        print(f'Number of transcations: {number_trans}, Wins: {wins}, Lose: {lose}, Defeat: {defeat} ')\n",
    "\n",
    "        # Save the model state_dict\n",
    "        torch.save(policy_net.state_dict(), model_save_path)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
